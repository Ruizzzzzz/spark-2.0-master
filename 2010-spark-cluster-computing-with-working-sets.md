# 摘要

MapReduce 及其变体在商品集群上实现大规模数据密集型应用程序方面非常成功。然而，这些系统中的大多数建立在不适合于其他流行应用的非循环数据流模型。本文针对一类这样的应用程序：那些在多个并行操作中重用一组工作数据的应用程序。这包括许多迭代机器学习算法，以及交互式数据分析工具。我们提出了一个名为 Spark 的新框架，支持这些应用程序，同时保留 MapReduce 的可扩展性和容错性。为了实现这些目标，Spark 引入了一个称为弹性分布式数据集（RDDs）的抽象。 RDD是跨一组机器分区的对象的只读集合，如果分区丢失，可以重新构建。 Spark 在迭代机器学习作业中的性能优于 Hadoop 10倍，并且可用于以亚秒级响应时间交互式查询 39GB 数据集。

# 1. 前言

集群计算的新模型已经变得广泛流行，其中通过自动提供位置感知调度，容错和负载平衡的系统在不可靠机器的集群上执行数据并行计算。 MapReduce开创了这个模型，而像 Dryad 和 Map-Reduce-Merge 这样的系统推广了支持的数据流类型。这些系统通过提供编程模型来实现其可扩展性和容错性，其中用户创建非循环数据流图以通过一组运算符传递输入数据。这允许底层系统管理调度和对故障做出反应，而无需用户干预。虽然该数据流编程模型对于大类应用程序是有用的，但是存在不能作为非循环数据流有效地表示的应用。在本文中，我们专注于一类这样的应用程序：那些在多个并行操作中重用一组工作数据的应用程序。这包括两个用例，我们已经看到 Hadoop 用户报告 MapReduce 不足：

* **Iterative jobs（迭代的任务）**：许多常见机器学习算法对同一数据集重复应用函数以优化参数（例如，通过梯度计算）。虽然每个迭代可以表示为 MapReduce / Dryad 作业，但每个作业必须从磁盘重新加载数据，从而产生显着的性能。
* **Interactive analytics（交互式分析）**：Hadoop 通常用于通过 SQL 接口（如 Pig 和 Hive ）对大型数据集运行 ad-hoc 探索性查询。理想情况下，用户将能够在多个机器上将感兴趣的数据集加载到内存中并重复查询。但是，使用 Hadoop，每个查询都会产生显着的延迟（几十秒），因为它作为一个单独的 MapReduce 作业运行，并从磁盘读取数据。本文提出了一个新的集群计算框架，称为 Spark，它支持应用程序与工作集，同时提供类似 MapReduce 的可扩展性和容错属性。

Spark 中的主要抽象是一个弹性分布数据集（RDD），它代表在一组机器上分区的对象的只读集合，如果分区丢失，可以重新构建这些对象。用户可以在内存中跨机器显式缓存 RDD，并在多个类似 MapReduce 的并行操作中重用它。 RDD 通过谱系概念实现容错：如果 RDD 的分区丢失，RDD 具有足够的信息，关于它是如何从其他 RDD 派生的，以便能够仅重建该分区。尽管 RDD 不是一般的共享内存抽象，但它们一方面表现在表达性之间，另一方面表示可扩展性和可靠性，我们发现它们非常适合各种应用。

Spark 在 Scala 中实现，它是 Java VM 的静态类型的高级编程语言，并且公开了一个类似于 DryadLINQ 的功能编程接口。此外，Spark 可以从 Scala 解释器的修改版本中主动使用，这允许用户定义 RDDs，函数，变量和类，并在集群上的并行操作中使用它们。我们相信 Spark是第一个允许高效的通用编程语言以交互方式处理集群上的大型数据集的系统。

虽然我们的 Spark实现仍然是一个原型，早期的系统经验是令人鼓舞的。我们表明Spark可以在机器学习工作负载中超过 Hadoop 10倍，并且可以交互使用以亚秒级延迟扫描 39GB 的数据集。

本文组织如下。第2节描述了 Spark 的编程模型和 RDD。第3节显示了一些示例作业。第4节描述了我们的实现，包括我们与 Scala及其解释器的集成。第5节提出早期结果。我们在第6节调查相关工作，最后在第7节讨论。

# 2. 编程模型

要使用 Spark，开发人员编写一个驱动程序，实现其应用程序的高级控制流程，并并行启动各种操作。 Spark 为并行编程提供了两个主要的抽象：弹性分布式数据集和对这些数据集的并行操作（通过传递一个函数应用于数据集来调用）。此外，Spark 支持两种限制类型的共享变量，这些变量可以在集群上运行的函数中使用，我们将在后面解释。

### 2.1 Resilient Distributed Datasets \(RDDs\)

弹性分布式数据集（RDD）是跨一组机器分区的对象的只读集合，如果分区丢失，可以重新构建这些对象。 RDD 的元素不需要存在于物理存储中;相反，RDD 的句柄包含足够的信息来计算从可靠存储中的数据开始的 RDD。这意味着如果节点失败，RDD 总是可以重建。

在 Spark 中，每个 RDD 由一个 Scala 对象表示。 Spark 让程序员以四种方式构造 RDD：

* 从共享文件系统中的文件，如 Hadoop 分布式文件系统（HDFS）。

* 通过在驱动程序中“并行化” Scala 集合（例如，数组），这意味着将其划分为将被发送到多个节点的多个片。

* 通过转换现有的 RDD。可以使用称为 flatMap 的操作将具有类型A的元素的数据集转换为具有类型B的元素的数据集，其通过用户提供的类型A⇒列表\[B\]的函数传递每个元素。 其他变换可以使用 flatMap 来表示，包括 map（通过类型A⇒B的函数的 pass 元素）和 filter（匹配谓词的 pick 元素）。

* 通过更改现有 RDD 的持久性。默认情况下，RDD 是懒惰和短暂的。也就是说，当数据集的分区在并行操作中使用（例如，通过 map 函数传递文件的块）时，数据集的分区可以根据需要实现，并在使用后从内存中分离。 但是，用户可以通过两个操作更改 RDD 的持久性：

  * 缓存操作使数据集延迟，但暗示它应该在第一次计算之后保存在内存中，因为它将被重用。

  * 保存操作将评估数据集并将其写入分布式文件系统（如 HDFS）。保存的版本用于将来的操作。

我们注意到，我们的缓存操作只是一个提示：如果集群中没有足够的内存来缓存数据集的所有分区，Spark将在使用时重新计算它们。我们选择了这种设计，使得Spark程序在节点故障或数据集太大时仍能继续工作（性能降低）。这个想法是松散地类似于虚拟内存。

我们还计划扩展 Spark 以支持其他级别的持久性（例如，跨多个节点的内存中复制）。我们的目标是让用户在存储 RDD 的成本，访问它的速度，丢失部分的可能性和重新计算它的成本之间进行权衡。

### 2.2 Parallel Operations

可以对 RDD 执行多个并行操作：

* **reduce**：使用相关函数组合数据集元素，以在驱动程序中产生结果。
* **collect**：将数据集的所有元素发送到驱动程序 driver 端。例如，一种并行更新数组的简单方法是并行化，映射和收集数组。
* **foreach**：通过用户提供的函数传递每个元素。这仅仅是为了函数的副作用（可能是将数据复制到另一个系统上，更新到另一个可变的变量）。

我们注意到 Spark 目前不支持 MapReduce 中的分组 reduce 操作; reduce 的结果只在一个进程（驱动程序 drive r端）收集。 如第7节所述，我们计划使用分布式数据集上的“随机”变换支持未来的分组减少。然而，即使使用单个减少器也足以表达各种有用的算法。例如，最近一篇关于多核系统上机器学习 MapReduce 的文章实现了10种不支持并行还原的学习算法。

### 2.3 Shared Variables

程序员通过将闭包（函数）传递给 Spark 来调用 map，filter 和 reduce 等操作。正如在函数式编程中，这些闭包可以引用创建它们的作用域中的变量。通常，当 Spark 在工作节点上运行闭包时，这些变量将复制到工作线程。但是，Spark 还允许程序员创建两种限制类型的共享变量，以支持两种简单但常见的使用模式：

* **Broadcast variables**：如果在多个并行操作中使用大的只读数据（例如，查找表），则优选地将它仅分发给 worker 一次，而不是将其与每个闭包一起包装。 Spark 让程序员创建一个包含该值的“广播变量”对象，并确保它只被复制到每个 worker 一次。
* **Accumulators**：这些 worker 只能“添加”到使用关联操作的变量，并且只有驱动程序可以读取。它们可以用于实现 MapReduce 中的计数器，并为并行和提供更强制的语法。可以为具有 “add” 运算和 “zero” 值的任何类型定义累加器。由于它们的“仅添加”语义，它们更易于容错。

# 3. Examples

我们现在显示一些示例 Spark 程序。注意，我们省略变量类型，因为 Scala 支持类型推断。

### 3.1 Text Search

假设我们希望计算存储在 HDFS 中的大型日志文件中包含错误的行。这可以通过从文件数据集对象开始实现，如下所示：

```scala
val file = spark.textFile("hdfs://...")

    val errs = file.filter(_.contains("ERROR"))

    val ones = errs.map(_ => 1)

    val count = ones.reduce(_+_)
```

我们首先创建一个名为 file 的分布式数据集，表示 HDFS file as a collection of lines。我们转换此数据集以创建包含 “ERROR”（errs）的行集，然后将每行映射到1，并使用 reduce 将这些行相加。 filter，map 和 reduce 的参数是函数文字的 Scala 语法。

请注意，err 和 ones 是从未实现的延迟 RDD。相反，当调用 reduce 时，每个 worker 节点以流的方式扫描输入块以评估它们，添加这些以执行本地缩减，并将其本地计数发送到驱动器。当以这种方式与懒数据集一起使用时，Spark 会密切地模拟 MapReduce。

Spark 与其他框架不同的地方在于，它可以使一些中间数据集在操作中保持不变。例如，如果想重用 errs 数据集，我们可以从它创建一个缓存的 RDD 如下：

```scala
val cachedErrs = errs.cache()
```

我们现在将能够像往常一样对 cachedErrs 或从它派生的数据集调用并行操作，但是节点会在第一次计算它们后缓存 cachedErrs 的分区到内存中，从而大大加快了对它的后续操作。

### 3.2 Logistic Regression

以下程序实现逻辑回归，一种迭代分类算法，试图找到最佳分离两组点的超平面 w。该算法执行梯度下降：它以随机值开始 w，并且在每次迭代中，它对数据的 w 的函数求和以在改进它的方向上移动 w。因此，它通过跨越迭代在内存中缓存数据极大地受益。我们不详细解释逻辑回归，但我们使用它来展示一些新的 Spark 特性。

```scala
// Read points from a text file and cache them

    val points = spark.textFile(...)

    .map(parsePoint).cache()

    // Initialize w to random D-dimensional vector

    var w = Vector.random(D)

    // Run multiple iterations to update w

    for (i <- 1 to ITERATIONS) {

        val grad = spark.accumulator(new Vector(D))

        for (p <- points) { // Runs in parallel

            val s = (1/(1+exp(-p.y * (w dot p.x)))-1) * p.y

            grad += s * p.x
        }
        w -= grad.value
    }
```

首先，虽然我们创建一个 RDD 称为点，我们通过运行一个 for 循环来处理它。 Scala 中的关键字是用于调用集合的 foreach 方法的语法糖，其中循环体作为闭包。也就是说，（p &lt; - points）{body} 的代码等同于 points.foreach（p =&gt;{body}）。因此，我们调用 Spark 的并行 foreach 操作。

第二，为了总结梯度，我们使用一个称为 gradient 的累加器变量（具有类型 V ector 的值）。请注意，循环使用重载的 + = 运算符添加到渐变。累加器和语法的组合允许 Spark 程序看起来很像强制性串行程序。事实上，这个例子不同于只有三行的逻辑回归的串行版本。

### 3.3 Alternating Least Squares

我们的最后一个例子是称为交替最小二乘法（ALS）的算法。 ALS 用于协作过滤问题，诸如基于他们的电影评级历史（如在 Netflix 挑战中）预测用户对未看到的电影的评级。与我们以前的例子不同，ALS 是 CPU 密集型而不是数据密集型。

我们简要描述 ALS，请自行参考官方 ALS 的细节。假设我们想要预测 u 个用户对 m 个电影的评分，并且我们有一个部分填充的矩阵 R 包含一些用户电影对的已知评级。 ALS 将 R 分别作为尺寸为 m×k 和 k×u 的两个矩阵 M 和 U 的乘积;也就是说，每个用户和每个电影具有描述其特征的k维“特征向量”，并且用户对电影的评级是其特征向量和电影的点积。 ALS 使用已知的标准求解 M 和 U，然后计算 M×U 以预测未知的标准。这是使用以下迭代过程完成的：

1.将 M 初始化为随机值。

2.优化 U 给定 M 以最小化 R 上的误差。

3.优化 M 给定 U 以最小化 R 上的误差。

4.重复步骤2和3直到收敛。

可以通过在步骤2和3中更新每个节点上的不同用户/电影来并行化 ALS。然而，由于所有步骤都使用 R，因此使 R 成为广播变量并且不会在每个步骤重新发送到每个节点是有帮助的。 ALS 的 Spark 实现如下所示。 注意我们并行化集合 0 直到 u（Scala范围对象），并收集它来更新每个数组：

```scala
val Rb = spark.broadcast(R)

    for (i <- 1 to ITERATIONS) {

        U = spark.parallelize(0 until u)

        .map(j => updateUser(j, Rb, M))

        .collect()

        M = spark.parallelize(0 until m)

        .map(j => updateUser(j, Rb, U))

        .collect()

    }
```

# 4. Implementation

Spark构建在 Mesos 之上，它是一个“集群操作系统”，它允许多个并行应用程序以细粒度的方式共享一个集群，并为应用程序在集群上启动任务提供API。这允许 Spark 与现有的集群计算框架一起运行，例如 Mesos ports of Hadoop and MPI，并与它们共享数据。此外，在 Mesos 上构建大大减少了必须进入Spark的编程工作。

Spark 的核心是实现弹性分布的数据集。例如，假设我们在日志文件中定义了一个名为 cachedErrs 的缓存数据集，它表示错误消息，并且我们使用 map 和 reduce 来计算其元素，如第3.1节所述：

```scala
    val file = spark.textFile("hdfs://...")

    val errs = file.filter(_.contains("ERROR"))

    val cachedErrs = errs.cache()

    val ones = cachedErrs.map(_ => 1)

    val count = ones.reduce(_+_)
```

这些数据集将被存储为一个对象链，捕获每个RDD的lineage，如图1所示。每个数据集对象包含一个指向父对象的指针以及有关父对象如何转换的信息。

![](/img/spark_paper/2010 Spark Cluster Computing with Working Sets/figure1.png)

在内部，每个RDD对象实现相同的简单接口，包括三个操作：

* getPartitions，它返回分区ID的列表。

* getIterator（partition），它遍历分区。

* getPreferredLocations（partition），用于任务调度以实现数据局部性。

当创建一个任务时，Spark 创建一个任务来处理数据集的每个分区，并将这些任务发送到工作节点。我们尝试使用称为延迟调度的技术将每个任务发送到其首选位置之一。一旦在工作程序上启动，每个任务调用 getIterator 以开始读取其分区。

不同类型的RDD仅在如何实现RDD接口方面有所不同。例如，对于 Hdfs-TextFile，分区是 HDFS 中的块 ID，它们优先的位置是块位置，getIterator 打开一个流来读取块。在 MappedDataset 中，分区和首选位置与父对象相同，但迭代器将映射函数应用于父对象的元素。最后，在 CachedDataset 中，getIterator 方法查找已转换分区的本地缓存副本，每个分区的首选位置开始等于父级的首选位置，但在某些节点上缓存分区后更新，以优先重用该节点。此设计使故障易于处理：如果节点故障，则从其父数据集重新读取其分区，并最终在其他节点上缓存。

最后，向工作者发送任务需要向他们发送闭包 - 用于定义分发数据集的闭包和传递给诸如 reduce 的操作的闭包。为了实现这一点，我们依赖于这样的事实，Scala cloures 是 Java 对象，可以使用Java集合序列化;这种情况下，相对直接的向上计算。然而，Scala的内置闭包实现并不理想，因为我们发现一个闭包对象引用闭包外部范围中实际上未在其主体中使用的变量的情况。我们已经提交了一个关于这个的错误报告，但是在此期间，我们已经解决了这个问题，通过对闭包类的字节码进行静态分析来检测这些未使用的变量，并将闭包对象中的相应字段设置为 null。由于缺乏空间，我们省略了该分析的细节。

**Shared Variables：**Spark 中的两种类型的共享变量，广播变量和累加器，使用具有自定义序列化格式的类来实现。当创建具有值 v 的广播变量 b 时，v 被保存到共享文件系统中的文件。 b 的序列化形式是此文件的路径。当在工作节点上查询 b 的值时，Spark 首先检查 v 是否在本地缓存中，如果不在本地缓存中，则从文件系统中读取。我们最初使用 HDFS 来广播变量，但是我们正在开发更高效的流式广播系统。

使用不同的“训练技巧”来实现累加器。每个累加器在被创建时被赋予唯一的ID。当保存累加器时，其序列化形式包含其ID和其类型的“零”值。在工作线程上，为使用线程局部变量运行任务的每个线程创建一个单独的累加器副本，并在任务开始时复位为零。每个任务运行后，工作者向驱动程序发送一条消息，其中包含它对各种累加器的更新。驱动程序仅应用来自每个操作的每个分区的更新一次，以防止在由于故障重新执行任务时进行重复计数。

**Interpreter Integration**：由于缺少空间，我们只描述我们如何将 Spark 集成到 Scala 解释器中。 Scala 解释器通常通过为用户键入的每一行编译一个类来操作。这个类包括一个单例对象，该对象包含该行上的变量或函数，并在其构造函数中运行该行的代码。例如，如果用户键入 var x = 5，然后按 println（x），解释器会定义一个包含 x 的类（称为 Line1），并使第二行转换为println（Line1.getInstance（）.x ）。这些类被加载到JVM中以运行每一行。为了使解释器与 Spark 协同工作，我们做了两个修改：

1.我们将解释器输出的类定义为共享文件系统，从中可以使用自定义Java类加载器由 workers 加载它们。

2.我们更改了生成的代码，以便每行的单例对象直接引用前一行的单例对象，而不是通过静态 getInstance 方法。这允许 clolock 来捕获他们引用的单例的当前状态，当它们被序列化以发送给工作者时。如果我们没有这样做，那么对单例对象的更新（例如，上面例子中的行设置 x = 7）不会传播给工人。

# 5. Results

虽然我们的 Spark 的实现仍处于早期阶段，我们将三个实验的结果联系起来，这三个实验显示了它作为一个集群计算框架的承诺。

**Logistic Regression：**我们将第3.2节中的逻辑回归工作的性能与 Hadoop 的逻辑回归的实现进行了比较，在 20 个 “m1.xlarge”EC2 节点上使用 29GB 数据集，每个节点有4个核心。结果如图2所示。对于 Hadoop，每次迭代需要127秒，因为它作为一个独立的 MapReduce 作业运行。使用 Spark，第一次迭代需要174秒（可能是由于使用Scala而不是Java），但是后续迭代只需要6秒，每次迭代都会重复使用缓存的数据。这允许作业运行速度快10倍。

![](/img/spark_paper/2010 Spark Cluster Computing with Working Sets/figure2.png)

我们也试图在作业运行时崩溃一个节点。在10次迭代的情况下，这会将作业平均减慢50秒（21％）。丢失节点上的数据分区在其他节点上重新计算并缓存，但在当前实验中恢复时间相当高，因为我们使用高 HDFS 块大小（128 MB），因此每个只有12个块节点，恢复过程无法使用群集中的所有核心。较小的块大小将产生更快的恢复时间。

**Alternating Least Squares：**我们已经在第3.3节中实现了交替最小二乘法作业，以测量广播变量对将共享数据集复制到多个节点的迭代作业的好处。我们发现，在不使用广播变量的情况下，在每次迭代中重新发送评级矩阵 R 的时间主导着作业的运行时间。此外，随着广播（使用 HDFS 或 NFS）的广泛实施，广播时间随着节点数量的增长而线性增长，从而限制了作业的可扩展性。我们实施了一个应用级多播系统来减轻这种情况。然而，即使使用快速广播，每次迭代的重发 R 也是昂贵的。在使用广播变量的工作器上的内存中缓存 R 在使用5000个电影和15个用户在30个节点的 EC2 集群上的实验中，性能提高了2.8倍。

**Interactive Spark：**我们使用 Spark 解释器在15个 “m1.xlarge” EC2 机器上在内存中加载一个 39GB 的维基百科转储，并以交互方式查询它。第一次查询数据集时，大约需要35秒，与在其上运行 Hadoop 作业相当。然而，后续查询只需要0.5到1秒，即使它们扫描所有数据。这提供了一种质量上不同的经验，与使用本地数据相当。

# 6. Related Work

**Distributed Shared Memory：**Spark 的弹性分布式数据集可以看作是分布式共享内存（DSM）的抽象，已经进行了广泛的研究。 RDD 与 DSM 接口在两个方面不同。首先，RDD 提供了一个更受限制的5编程模型，但是如果集群节点失败，则允许数据集被有效地重建。虽然一些 DSM 系统通过检查点实现容错，但 Spark 使用 RDD 对象中捕获的沿袭信息重建 RDD 的丢失分区。这意味着只有丢失的分区需要重新计算，并且它们可以在不同节点上并行重新计算，而不需要程序恢复到检查点。此外，如果没有节点故障，则没有开销。其次，RDDs 像 MapReduce 一样将计算推送到数据，而不是让任意节点访问全局地址空间。

其他系统也限制了 DSM 编程模型，以提高性能，可靠性和可编程性。 Munin 让程序员用访问模式注释变量，他们将为它们选择一个最优的一致性协议。 Linda 提供了一个可以以容错方式实现的元组空间编程模型。 Thor 提供了一个持久共享对象的接口。

**Cluster Computing Frameworks：**Spark 的并行操作适合 MapReduce 模型。然而，它们对可以在操作中持续的 RDD 操作。

Twister 也认识到需要扩展 MapReduce 以支持迭代作业，MapReduce 框架允许长期map任务在作业之间在内存中保留静态数据。但是，Twister 当前不实现容错。 Spark 对弹性分布式数据集的抽象是容错的和更生气勃勃的迭代 MapReduce。 Spark 程序可以定义多个 RDD 并在它们之间交替运行操作，而 Twister 程序只有一个 map 函数和一个 reduce 函数。这也使得 Spark 对于交互式数据分析非常有用，用户可以在其中定义多个数据集，然后查询它们。

Spark 的广播变量为 Hadoop 的分布式缓存提供了类似的功能，可以将文件传播到运行特定作业的所有节点。然而，广播变量可以在并行操作中重用。

**Language Integration：**Spark 的语言集成类似于 DryadLINQ，它使用 .NET 对语言集成查询的支持来捕获定义查询并在集群上运行它的表达式树。与 DryadLINQ 不同，Spark 允许 RDD 在并行操作中持久存储在内存中。此外，Spark 通过支持使用具有自定义序列化形式的类实现的共享变量（广播变量和累加器）丰富了语言集成模型。

我们被启发使用 Scala 用于 SMR 的语言集成，这是一个使用 clo 来定义 map 和 reduce 任务的 Hadoop 的 Scala 接口。我们对 SMR 的贡献是共享变量和更可靠的实施保护串行化（在第4节中描述）。

最后，IPython 是科学家的 Python 解释器，它允许用户使用容错任务队列接口或低级消息传递接口在集群上启动计算。 Spark 提供了类似的交互式接口，但重点在于数据密集型计算。

**Lineage：**捕获数据集的血统或起源信息长期以来一直是科学计算数据库领域的研究课题，用于解释结果，允许他们被其他人复制以及如果在工作中发现错误时重新计算数据等应用，流步骤或数据集丢失。我们引用读者\[7\]，\[23\]和\[9\]来调查这项工作。 Spark 提供了一种限制的并行编程模型，其中细粒度血统捕获成本低，因此该信息可用于重新计算丢失的数据集元素。

# 7. Discussion and Future Work

Spark 为编程集群提供了三种简单的数据抽象：弹性分布式数据集（RDDs）和两种限制类型的共享变量：广播变量和累加器。虽然这些抽象是有限的，但我们发现它们足够强大，足以表达对现有集群计算框架（包括迭代和交互式计算）构成挑战的多个应用程序。此外，我们认为，RDDs 背后的核心思想是一个数据集句柄，它具有足够的信息来从可靠的存储中可用的数据中重建数据集，这对于开发编程集群的其他抽象可能是有用的。

在未来的工作中，我们计划重点关注四个方面：

1.正式表征 RDDs 和 Spark 的其他属性，以及各种应用和工作负载的适用性。

2.增强 RDD 抽象以允许程序员在存储成本和重建成本之间进行交换。

3.定义新操作来转换 RDD，包括通过给定键重新分配 RDD 的 “shuffle” 操作。这样的操作将允许我们实现分组和联接。

4.在 Spark 解释器之上提供更高级别的交互式接口，例如SQL和R shell。

# 8. Acknowledgements

我们感谢 Ali Ghodsi 对本文的反馈。这项研究由加利福尼亚 MICRO，加利福尼亚自然科学和加拿大工程研究理事会以及以下 Berkeley RAD 实验室赞助者支持：Sun Microsystems，Google，微软，亚马逊，思科， Cloudera，eBay，Facebook，富士通，惠普，英特尔，NetApp，SAP，VMware和雅虎！


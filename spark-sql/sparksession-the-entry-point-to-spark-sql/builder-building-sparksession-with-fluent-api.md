## `Builder` — Building`SparkSession`with Fluent API {#__a_id_builder_a_code_builder_code_building_code_sparksession_code_with_fluent_api}

Builder是构建完全配置的SparkSession的fluent API。

Table 1. Builder Methods

| Method | Description |
| :---: | :---: |
| enableHiveSupport | Enables Hive support. |

```
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .getOrCreate
```

您可以使用流畅的设计模式设置将会话打开到Spark SQL的SparkSession的各种属性。

| Note | 您可以在单个Spark应用程序中为不同的数据目录（通过关系实体）拥有多个SparkSession。 |
| :---: | :--- |


### `config`method {#__a_id_config_a_code_config_code_method}

| Caution | FIXME |
| :--- | :--- |


### Enabling Hive Support — `enableHiveSupport`Method {#__a_id_enablehivesupport_a_enabling_hive_support_code_enablehivesupport_code_method}

创建SparkSession时，可以选择使用enableHiveSupport方法启用Hive支持。

```
enableHiveSupport(): Builder
```

enableHiveSupport启用Hive支持（与持久Hive Metastore的连接，Hive Serdes的支持以及Hive用户定义的函数）。

| Note | 您不需要任何现有的Hive安装来使用Spark的Hive支持。 SparkSession上下文将在Spark应用程序的当前目录和spark.sql.warehouse.dir配置的目录中自动创建metastore\_db。 |
| :---: | :--- |


在内部，enableHiveSupport确保Hive类在CLASSPATH上，即Spark SQL的org.apache.spark.sql.hive.HiveSessionState和org.apache.hadoop.hive.conf.HiveConf，并将spark.sql.catalogImplementation属性设置为hive 。














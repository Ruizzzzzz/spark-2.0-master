## `SparkSession` — The Entry Point to Spark SQL {#__a_id_sparksession_a_code_sparksession_code_the_entry_point_to_spark_sql}

SparkSession是Spark SQL的入口点。这是您必须创建的第一个对象，以使用完全类型化的Dataset（和无类型的DataFrame）数据抽象开始开发Spark SQL应用程序。

| Note | SparkSession在Spark 2.0.0中的一个对象中合并了SQLContext和HiveContext。 |
| :---: | :--- |


您可以使用SparkSession.builder方法创建SparkSession的实例。

```
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .getOrCreate
```

并使用stop方法停止当前的SparkSession。

```
spark.stop
```

您可以在单个Spark应用程序中拥有多个SparkSession。

在内部，SparkSession需要一个SparkContext和一个可选的SharedState（表示跨SparkSession实例的共享状态）。

Table 1.`SparkSession`Class and Instance Methods

| Method | Description |
| :--- | :--- |
| builder | "Opens" a builder to get or create a`SparkSession`instance |
| version | Returns the current version of Spark. |
| implicits | Use`import spark.implicits._`to import the implicits conversions and create`Datasets`from \(almost arbitrary\) Scala objects. |
| emptyDataset\[T\] | Creates an empty`Dataset[T]`. |
| range | Creates a`Dataset[Long]`. |
| sql | Executes a SQL query \(and returns a`DataFrame`\). |
| udf | Access to user-defined functions \(UDFs\). |
| table | Creates a`DataFrame`from a table. |
| catalog | Access to the catalog of the entities of structured queries |
| read | Access to`DataFrameReader`to read a`DataFrame`from external files and storage systems. |
| conf | Access to the current runtime configuration. |
| readStream | Access to`DataStreamReader`to read streaming datasets. |
| streams | Access to`StreamingQueryManager`to manage structured streaming queries. |
| newSession | Creates a new`SparkSession`. |
| stop | Stops the`SparkSession`. |

| Tip | Use spark.sql.warehouse.dir Spark property to change the location of Hive’s`hive.metastore.warehouse.dir`property, i.e. the location of the Hive local/embedded metastore database \(using Derby\).                                                                         Refer to SharedState in this document to learn about \(the low-level details of\) Spark SQL support for Apache Hive.      See also the official Hive Metastore Administration document. |
| :--- | :--- |


### Creating`SparkSession`Using`Builder` — `builder`method {#__a_id_builder_a_creating_code_sparksession_code_using_code_builder_code_code_builder_code_method}

```
builder(): Builder
```

构建器创建一个新的 builder，用于使用fluent的API构建完全配置的SparkSession

```
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
```

### Accessing Version of Spark — `version`Method {#__a_id_version_a_accessing_version_of_spark_code_version_code_method}

```
version: String
```

`version`返回正在使用的Apache Spark的版本。

在内部，版本使用spark.SPARK\_VERSION值，该值是CLASSPATH上的spark-version-info.properties属性文件中的version属性。

### Implicit Conversions — `implicits`object {#__a_id_implicits_a_implicit_conversions_code_implicits_code_object}

implicits对象是一个具有Scala隐式方法（也称为转换）的辅助类，用于将Scala对象转换为Datasets，DataFrames和Columns。它还定义了Scoders的“原始”类型的编码器，例如Int，Double，String，以及它们的产品和集合。

Import the implicits by`import spark.implicits._`.

```
val spark = SparkSession.builder.getOrCreate()
import spark.implicits._
```

implicits对象支持从任何类型的RDD（范围中存在编码器）或案例类或元组和Seq创建Dataset。

implicits object还提供从Scala的Symbol或$到Column的转换。

它还提供从RDD或Seq的产品类型（例如案例类或元组）到DataFrame的转换。它具有从Int，Long和String的RDD到具有单个列名称\_1的DataFrame的直接转换。

| Note | 只能对Int，Long和String“primitive”类型的RDD对象调用ofDF方法。 |
| :---: | :--- |


### Creating Empty Dataset — `emptyDataset`method {#__a_id_emptydataset_a_creating_empty_dataset_code_emptydataset_code_method}

```
emptyDataset[T: Encoder]: Dataset[T]
```

emptyDataset创建一个空的数据集（假设未来的记录类型为T）。

```
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
```

emptyDataset创建一个LocalRelation逻辑查询计划。

### Creating Dataset from Local Collections and RDDs — `createDataset`methods {#__a_id_createdataset_a_creating_dataset_from_local_collections_and_rdds_code_createdataset_code_methods}

```
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
```

createDataset是从本地Scala集合（即Seq \[T\]，Java's List \[T\]或分布式RDD \[T\]）创建数据集的实验API。

```
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
```

createDataset创建一个LocalRelation逻辑查询计划（对于输入数据集合）或LogicalRDD（对于输入RDD \[T\]）。

你最好使用Scala隐式和toDS方法（这是自动为你的转换）。

```
val spark: SparkSession = ...
import spark.implicits._

scala> val one = Seq(1).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
```

在内部，createDataset首先查找范围中的隐式表达式编码器以访问AttributeReferences（模式的）。

| Note | 目前仅支持未解析的表达式编码器。 |
| :---: | :--- |


然后使用表达式编码器将（输入Seq \[T\]的）元素映射到InternalRows的集合中。通过引用和行，createDataset返回具有LocalRelation逻辑查询计划的Dataset。

### Creating Dataset With Single Long Column — `range`methods {#__a_id_range_a_creating_dataset_with_single_long_column_code_range_code_methods}

```
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
```

`range`family of methods create a Dataset of`Long`numbers.

```
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
```

| Note | The three first variants \(that do not specify`numPartitions`explicitly\) use SparkContext.defaultParallelism for the number of partitions`numPartitions`. |
| :--- | :--- |


在内部，range使用Range逻辑计划和Encoders.LONG编码器创建一个新的Dataset \[Long\]。

### Creating Empty DataFrame — `emptyDataFrame`method {#__a_id_emptydataframe_a_creating_empty_dataframe_code_emptydataframe_code_method}

```
emptyDataFrame: DataFrame
```

emptyDataFrame创建一个空DataFrame（没有行和列）。

它调用带有空RDD \[Row\]和空模式StructType（Nil）的createDataFrame。

### Creating DataFrames from RDDs with Explicit Schema — `createDataFrame`method {#__a_id_createdataframe_a_creating_dataframes_from_rdds_with_explicit_schema_code_createdataframe_code_method}

```
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
```

createDataFrame使用RDD \[Row\]和输入模式创建DataFrame。假设rowRDD中的行都与模式匹配。

### Executing SQL Queries — `sql`method {#__a_id_sql_a_executing_sql_queries_code_sql_code_method}

```
sql(sqlText: String): DataFrame
```

`sql`executes the`sqlText`SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

在内部，它使用当前的SparkSession和一个逻辑计划创建一个Dataset。通过使用sessionState.sqlParser解析输入sqlText来创建计划。

### Accessing UDF Registration Interface — `udf`Attribute {#__a_id_udf_a_accessing_udf_registration_interface_code_udf_code_attribute}

```
udf: UDFRegistration
```

udf属性提供对UDFRegistration的访问，允许为基于SQL的查询表达式注册用户定义的函数。

```
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
```

在内部，它是SessionState.udf的别名。

### Creating DataFrames from Tables — `table`method {#__a_id_table_a_creating_dataframes_from_tables_code_table_code_method}

```
table(tableName: String): DataFrame
```

表根据tableName表中的记录创建DataFrame（如果存在）。

```
val df = spark.table("mytable")
```

### Accessing Metastore — `catalog`Attribute {#__a_id_catalog_a_accessing_metastore_code_catalog_code_attribute}

```
catalog: Catalog
```

catalog属性是当前元数据仓（即关系实体（如数据库，表，函数，表列和临时视图）的数据目录）的（惰性）接口。

| Tip | Catalog中的所有方法都返回Dataset。 |
| :---: | :--- |


```
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
```

在内部，catalog创建一个CatalogImpl（引用当前的SparkSession）。

### Accessing DataFrameReader — `read`method {#__a_id_read_a_accessing_dataframereader_code_read_code_method}

```
read: DataFrameReader
```

read方法返回一个DataFrameReader，用于从外部存储系统读取数据并将其加载到DataFrame中。

```
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
```

### Runtime Configuration — `conf`attribute {#__a_id_conf_a_runtime_configuration_code_conf_code_attribute}

```
conf: RuntimeConfig
```

conf返回包装SQLConf的当前运行时配置（作为RuntimeConfig）。

### `sessionState`Property {#__a_id_sessionstate_a_code_sessionstate_code_property}

sessionState是一个表示当前SessionState的临时惰性值。

| Note | sessionState是一个私有的\[sql\]值，所以你只能在org.apache.spark.sql包中的代码中访问它。 |
| :---: | :--- |


sessionState是一个基于内部spark.sql.catalogImplementation设置的延迟创建的值，可以是：

* `org.apache.spark.sql.hive.HiveSessionState`for`hive`

* `org.apache.spark.sql.internal.SessionState`for`in-memory`

### `readStream`method {#__a_id_readstream_a_code_readstream_code_method}

```
readStream: DataStreamReader
```

readStream返回一个新的DataStreamReader。

### `streams`Attribute {#__a_id_streams_a_code_streams_code_attribute}

```
streams: StreamingQueryManager
```

streams属性提供对StreamingQueryManager（通过SessionState）的访问。

```
val spark: SparkSession = ...
spark.streams.active.foreach(println)
```

### `streamingQueryManager`Attribute {#__a_id_streamingquerymanager_a_code_streamingquerymanager_code_attribute}

`streamingQueryManager`is…​

### `listenerManager`Attribute {#__a_id_listenermanager_a_code_listenermanager_code_attribute}

`listenerManager`is…​

### `ExecutionListenerManager` {#__a_id_executionlistenermanager_a_code_executionlistenermanager_code}

`ExecutionListenerManager`is…​

### `functionRegistry`Attribute {#__a_id_functionregistry_a_code_functionregistry_code_attribute}

`functionRegistry`is…​

### `experimentalMethods`Attribute {#__a_id_experimentalmethods_a_code_experimentalmethods_code_attribute}

```
experimental: ExperimentalMethods
```

experimentalMethods是一个扩展点，ExperimentalMethods是一个额外策略和Rule \[LogicalPlan\]的per-session集合。

| Note | 实验性用于SparkPlanner和SparkOptimizer。 Hive和Structured Streaming使用它自己的额外的策略和优化规则。 |
| :---: | :--- |


### `newSession`method {#__a_id_newsession_a_code_newsession_code_method}

```
newSession(): SparkSession
```

newSession创建（启动）一个新的SparkSession（具有当前的SparkContext和SharedState）。

```
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
```

### `sharedState`Attribute {#__a_id_sharedstate_a_code_sharedstate_code_attribute}

sharedState是当前的SharedState。第一次访问时会懒惰地创建它。

### `SharedState` {#__a_id_sharedstate_a_code_sharedstate_code}

SharedState是一个内部类，通过共享CacheManager，SQLListener和ExternalCatalog来保存活动SQL会话（作为SparkSession实例）的共享状态。

Enable`INFO`logging level for`org.apache.spark.sql.internal.SharedState`logger to see what happens inside.

Add the following line to`conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.internal.SharedState=INFO
```

SharedState在创建时需要一个SparkContext。它还将hive-site.xml添加到当前SparkContext中的Hadoop配置（如果在CLASSPATH上找到）。

| Note | hive-site.xml是在Spark中使用Hive时可选的Hive配置文件。 |
| :---: | :--- |


完全限定类名是org.apache.spark.sql.internal.SharedState。

SharedState是懒惰创建的，即在SparkSession创建后首次访问时。当创建新会话或访问共享服务时，可能会发生这种情况。它是用SparkContext创建的。

创建时，如果未设置hive.metastore.warehouse.dir或设置spark.sql.warehouse.dir，SharedState会将hive.metastore.warehouse.dir设置为spark.sql.warehouse.dir。否则，当设置hive.metastore.warehouse.dir并且不设置spark.sql.warehouse.dir时，spark.sql.warehouse.dir将设置为hive.metastore.warehouse.dir。您应该在日志中看到以下INFO消息：

```
INFO spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('[hiveWarehouseDir]').
```

You should see the following INFO message in the logs:

```
INFO SharedState: Warehouse path is '[warehousePath]'.
```

### Stopping SparkSession — `stop`Method {#__a_id_stop_a_stopping_sparksession_code_stop_code_method}

```
stop(): Unit
```

stop停止SparkSession，即停止底层的SparkContext。

### Creating`SparkSession`Instance {#__a_id_creating_instance_a_creating_code_sparksession_code_instance}

| Caution | FIXME |
| :--- | :--- |


### `baseRelationToDataFrame`Method {#__a_id_baserelationtodataframe_a_code_baserelationtodataframe_code_method}

| Caution | FIXME |
| :--- | :--- |











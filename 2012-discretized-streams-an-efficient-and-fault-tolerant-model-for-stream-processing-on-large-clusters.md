# 摘要

许多重要的“大数据”应用程序需要处理实时到达的数据。然而，用于分布式流处理的当前编程模型是相对低级的，通常使用户担心跨系统的状态一致性和故障恢复。此外，提供故障恢复的模型以昂贵的方式这样做，需要热复制或长的恢复时间。我们提出一个新的编程模型，离散流（D-Streams），提供高级功能编程API，强一致性和高效的故障恢复。 D-Streams 支持新的恢复机制，可提高流传输数据库中的传统复制和上游备份解决方案的效率：并行恢复群集中的丢失状态。我们在 Spark 集群计算框架（称为 Spark Streaming）的扩展中对 D-Streams 进行了原型设计，使用户可以无缝地混合流式处理，批处理和交互式查询。

# 1. Introduction

许多“大数据”被实时接收，并且在其到达时是最有价值的。例如，社交网络可能想要在几分钟内识别趋势对话主题，广告提供商可能想要训练用户点击新广告的模型，并且服务操作者可能想要挖掘日志文件以在数秒内检测故障。

为了处理它们涉及的数据量和计算量，这些应用需要在集群上分布。然而，尽管在批处理计算的集群编程模型方面做了大量工作，但是很少有类似的高级工具用于流处理。大多数当前的分布式流处理系统，包括 Yahoo 的 S4，Twitter 的 Storm 和流数据库，都是基于一个一次性记录处理模型，其中节点接收每个记录，更新内部状态，并发送新的记录作为响应。这个模型在大型云环境中带来了几个挑战：

* **Fault tolerance：**一次记录系统通过复制（其中每个处理节点有两个副本）或上游备份（其中节点缓冲发送的消息并将其恢复到故障下游节点的第二个副本）来提供恢复。这两种方法在大型集群中都不具有吸引力：复制需要 2× 硬件，并且如果两个节点故障，可能不工作，而上游备份需要很长时间来恢复，因为整个系统必须等待备用节点恢复故障节点的状态。
* **Consistency：**根据系统，可能很难推断全局状态，因为不同的节点可能正在处理在不同时间到达的数据。例如，假设系统对来自一个节点上的男性用户和来自另一个节点上的女性的页面视图进行计数。如果这些节点之一被积压，它们的计数器的比率将是错误的。
* **Unification with batch processing：**因为流系统的接口是事件驱动的，所以它与批处理系统的 API 截然不同，因此用户必须为每个分析任务编写两个版本。另外，难以将流数据与历史数据组合，例如，将事件流与历史数据相加以做出决定。

在这项工作中，我们提出了一个新的编程模型，离散流（D-Streams），克服了这些挑战。 D-Streams背后的关键理念是将流计算视为在小时间间隔上的一系列确定性批量计算。例如，我们可能将每秒接收到的数据放入一个新的间隔，并在每个间隔上运行一个 MapReduce 操作来计算一个计数。类似地，我们可以通过将每个间隔中的新计数添加到旧结果来执行多个间隔的运行计数。 D-Stream 模型的两个直接优点是一致性是良好定义的（每个记录以其到达的间隔进行原子处理），并且处理模型易于与批处理系统统一。此外，如我们将要示出的，我们可以使用类似的恢复机制来批量系统，尽管在更小的时间量级上，以比现有流传输系统更有效地减轻故障，即以更低的成本更快地恢复数据。

实现D-Stream模型有两个关键挑战。第一个是使延迟（间隔粒度）低。传统的批处理系统，如 Hadoop 和 Dryad，在这里，因为他们保持在磁盘上的工作之间的状态，并需要几十秒来运行每个工作。相反，为了满足几秒的目标延迟，我们保持在存储器中的中间状态。然而，由于数据复制的成本，将状态简单地置于通用存储器存储系统（例如键值存储器）中将是昂贵的。相反，我们建立在弹性分布式数据集（RDDs）上，这是一种存储抽象，可以通过跟踪重新计算它所需的操作来重建丢失的数据，而无需复制。除了支持小到100毫秒的任务的快速执行引擎（Spark），我们表明我们可以实现低至一秒的延迟。我们认为这对于许多现实世界的大数据应用是足够的，其中监视的事件的时间尺度（例如，社交网络中的趋势）高得多。

第二个挑战是从故障中迅速恢复。在这里，我们使用每个间隔中的批处理操作的确定性性质来提供在先前的流系统中没有存在的新的恢复机制：并行恢复丢失的节点的状态。集群中的每个节点都会重新计算丢失节点的 RDD 的一部分，导致比上游备份更快的恢复，而无需复制成本。由于即使对于基本复制也需要复杂的状态维护协议（例如，Flux ），但是在 D-Stream 的确定性模型中却很简单，所以在并行记录系统中难以实现并行恢复。

我们在 Spark Streaming 中创建了 D-Streams，这是 Spark 集群计算引擎的扩展。除了启用低延迟流处理之外，SparkStreaming 与 Spark 的批处理和交互处理功能相互操作，允许用户对到达的流运行即席查询或混合来自同一高级 API 的流和历史数据。

# 2. Discretized Streams \(D-Streams\)

我们的模型背后的关键想法是将流计算作为一系列确定性批处理计算在小的时间间隔。在每个间隔期间接收的输入数据可靠地存储在集群中以形成该间隔的输入数据集。一旦时间间隔完成，通过确定性并行操作（例如 map，reduce 和 groupBy）处理该数据集，以产生表示程序输出或中间状态的新数据集。我们将这些结果存储在弹性分布数据集（RDDs）中，这是一种有效的存储支持，通过使用沿袭来恢复故障，避免复制，我们将在后面解释。

离散流或 D-Stream 将一系列 RDD 组合在一起，并让用户通过各种运算符操作它们。 D 流提供无状态操作符，例如在每个时间间隔独立操作的映射，以及在多个间隔上操作并且可以产生中间 RDD 作为状态的状态操作符，例如在滑动窗口上的聚合。

我们用 Spark Streaming 程序来说明这个想法，该程序通过 URL 计算页面查看事件的运行计数。 Spark Streaming 提供了类似于 Scala 语言中的 DryadLINQ 的语言集成 API。视图计数程序的代码是：

```
pageViews = readStream（“http：// ...”，“1s”）

ones = pageViews.map（event =&gt;（event.url，1））

counts = ones.runningReduce（（a，b） b）
```

此代码通过 HTTP 读取事件流创建一个称为 pageViews 的 D 流，并将其分组为1秒间隔。然后它转换事件流以获得称为1的（URL，1）对的 D 流，并使用运行的 Reduce 运算符执行这些对的运行计数。map 和运行 Reduce 的参数是一个闭包（函数文字）的Scala语法。

最后，为了从故障中恢复，D-Streams 和 RDDs 都跟踪其沿袭，即用于构建它们的确定性操作集。我们跟踪这个信息为一个依赖图，类似于图1. 当一个节点失败时，我们通过重新运行map，减少等操作来重新计算它上面的 RDD 分区，用于在仍然在集群中的数据上构建它们。系统还周期性地检查状态 RDD（例如，通过复制每第五个 RDD）以防止无限重新计算，但这不需要对所有数据发生，因为恢复通常是快速的：丢失的分区可以在单独的节点上并行重新计算，我们将在第3节中讨论。

![](/img/spark_paper/2012 Discretized Streams An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters/figure1.png)  
**D-Stream Operators  **D-Streams提供两种类型的运算符，让用户构建流式传输程序：

* Transformation operators（转换运算符），其从一个或多个父流产生新的 D 流。这些可以是无状态的（即，独立地对每个间隔起作用）或有状态（跨间隔共享数据）。

* Output operators（输出运算符），它让程序将数据写入外部系统（例如，将每个 RDD 保存到 HDFS）。

D-Streams 支持典型的批处理框架中可用的相同的无状态转换，包括 map，reduce，groupBy 和 join。我们重用了 Spark 中的所有运算符。例如，程序可以使用以下代码在句子的 D 流的每个时间间隔上运行替代 MapReduce wordcount：

```
words = sentence.flatMap（s =&gt; s.split（“”））

pairs = words.map（w =&gt;（w，1））

counts = pairs.reduceByKey（（a，b）=&gt; a + b）
```

此外，D-Streams 引入了在多个间隔上工作的新的状态操作器。这些包括：

* **Windowing（窗口）：**窗口运算符将来自过​​去时间间隔范围的所有记录分组到单个 RDD 中。例如，在我们早期的代码中，调用pairs.window（“5s”）.reduceByKey（\_ + \_）在间隔 \[0,5），\[1,6\]，\[2 ，7）等。窗口是最通用的有状态操作符，但它也经常是低效的，因为它重复工作。
* **Incremental aggregation（增量聚合）：**对于在滑动窗口上计算聚合值（例如计数或总和）的常见使用情况，D-Streams 具有若干变量/窗口操作符。最简单的一个只需要一个关联的“合并”操作来组合值。例如，可以写：pairs.reduceByWindow（“5s”，（a，b）=&gt; a + b）这计算每个时间间隔的每间隔计数一次，但必须将计数过去五秒钟，如图2a所示。用于可逆聚合函数的更有效的版本还采用用于“减去”值并且递增地更新状态的函数（图2b）。
* **Time-skewed joins（时间偏移joins）：**用户可以在过去某段时间将流与其自己的 RDD 相加，以计算趋势，例如，当前页面浏览量与五分钟前的页面查看次数的比较情况。

![](/img/spark_paper/2012 Discretized Streams An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters/figure2.png)

最后，用户调用输出运算符以将结果从 D-Stream 传送到外部系统（例如，用于在仪表板上显示）。我们提供两个这样的运算符：save，它将每个 RDD 写入 D-Stream 到存储系统2和 foreach，它在流中的每个 RDD 上运行用户代码片段（任何 Spark 代码）。例如，用户可以打印以上计算的计数：counts.foreach（rdd =&gt; println（rdd.collect（）））使用批处理和交互式处理进行统一，因为 D-Streams 遵循与批处理系统相同的处理模型，可以自然结合。 Spark Streaming 提供了几个强大的功能来统一流式处理和批处理。

首先，D 流可以与例如通过加载文件计算的静态 RDD 组合。例如，可以针对预先计算的垃圾邮件过滤器加入进入的 tweet 的流，或者将其与历史数据进行比较。

第二，用户可以对先前的历史数据运行 D-Stream 程序作为批处理作业。这使得它很容易地计算一个关于过去的数据的新的流报告。

第三，用户可以将 Scala 控制台附加到 Spark Streaming 程序，以便使用 Spark 现有的快速交互式查询功能以交互方式对 D-Streams 运行即席查询。例如，用户可以通过键入以下命令查询时间范围内最常用的单词：counts.slice（“21:00”，“21:05”）topK（10）快速查询系统中任何状态的能力对于用户实时排查问题非常宝贵。

# 3. Fault Recovery

经典流传输系统在每个记录的基础上更新可变状态，并使用复制或上游备份进行故障恢复。

复制方法在数据流图中创建两个或多个每个进程的副本。因此，支持一个节点故障会使硬件成本翻倍。此外，如果同一副本中的两个节点失败，则系统不可恢复。由于这些原因，复制在我们的大规模云设置中不是成本效益好的。

在上游备份中，每个上游节点缓冲发送到下游节点的数据，直到它得到确认所有相关的计算完成。当节点故障时，上游节点将所有未确认的数据重新发送到备用节点，备用节点接管故障节点的角色并重新处理数据。这种方法的缺点是恢复时间长，因为系统必须等待备用节点赶上。

为了解决这些问题，D-Streams 采用了一种新的方法：并行恢复。系统通过异步地将它们复制到其他节点来周期性地检查某些状态 RDD。例如，在计算小时窗口的视图计数程序中，系统可以每分钟检查点结果。当节点发生故障时，系统会检测到丢失的 RDD 分区，并启动任务以从最新检查点恢复它们。许多细粒度任务可以同时启动，以计算不同节点上的不同 RDD 分区。因此，并行恢复比上传备份完成更快，成本比复制低得多。

为了显示这种方法的好处，我们从图3中的简单分析模型给出结果。该模型假设系统正在从一个分钟的检查点恢复，并且恢复过程中的瓶颈资源是CPU。在上游备份线中，单个空闲机器执行所有恢复，然后开始处理新记录。在高系统负载下需要很长时间才能赶上，因为它的新记录在重建旧状态时继续累积。 在其他行，所有的机器都参与恢复，同时还处理新的记录。随着节点越多，并行恢复的速度越快，到达流的速度比上游备份快得多。

![](/img/spark_paper/2012 Discretized Streams An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters/figure3.png)

在以前的流传输系统中难以执行并行恢复的一个原因是它们基于每个记录来处理数据，这甚至对于基本复制需要复杂和昂贵的bookkeeping 协议（例如，Flux）。相比之下，D-Streams 在 RDD 分区的更粗糙的粒度上应用确定性变换，这导致类似于批量数据流系统的更轻的 bookkeeping 和简单恢复。

最后，除了节点故障，大集群中的另一个重要的关注是 stragglers 。幸运的是，D-Streams 还可以通过执行慢任务的推测性备份副本，以与批量框架（如 MapReduce）相同的方式从堆栈中恢复。这种类型的推测在记录一次性系统中将再次是困难的，但是对于确定性任务变得简单。

# 4. Results

我们实现了一个 Spark Streaming 的原型，该原型扩展了现有的 Spark 运行时，并且可以从网络或从定期上传到 HDFS 的文件接收数据流。我们使用 Amazon EC2 上的实验简要评估了其可扩展性和故障恢复。我们使用具有4个内核和15 GB RAM的节点。

### Scalability（可扩展性）

我们通过两个应用程序评估系统的可扩展性：Grep（计算符合模式的输入记录）和 WordCount（在10秒钟的窗口中执行滑动窗口单词计数）。对于这两种应用，我们测量了在不同大小的群集上可实现的最大吞吐量，端到端延迟目标为1或2秒。通过端到端延迟，我们指的是记录进入系统之间和结果中出现的总时间，包括等待批处理开始的时间。我们使用0.5s的批处理间隔和100字节的记录。

图4绘制结果。我们看到，对于 WordCount，在亚秒级延迟下，系统可以为 Grep 处理大约 40MB /秒/节点（400K记录/秒/节点）和20MB /秒/节点（200K记录/以及稍微更多的数据，如果我们允许 2s 的延迟。系统也近似线性地缩放到50个节点。缩放不是完美的，因为有更多的节点会掉队。

![](/img/spark_paper/2012 Discretized Streams An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters/figure4.png)

### Parallel Recovery （并行恢复）

我们使用两个应用程序来评估并行故障恢复，两个应用程序在10个节点上每个节点接收10 MB / s的数据，并使用2秒的批处理间隔。第一个应用程序 MaxCount 在每个2秒间隔内执行一个字计数，并使用滑动窗口计算过去30秒内每个字的最大计数。因为max不是可逆操作，所以我们使用每2s重新计算一次的 reduceByWindow。我们运行这个应用程序，无需任何检查点（除了复制输入数据）。每个间隔在故障之前进行了 1.66s 的处理，而发生故障的间隔的平均处理时间为 2.64s（std.dev。0.19s）。即使必须重新计算30秒的结果，这是并行完成的，只花费一个额外的秒延迟。

第二个应用程序使用递增的 reduceByWindow 运算符和30秒的检查点数据执行一个具有30秒窗口的滑动字计数。这里，无故障间隔为1.47s，而故障间隔为平均 2.31s（std.dev 0.43s）。恢复速度比 Max-Count 快，因为每个时间间隔的输出只取决于前面三个 RDD（上一个间隔的总计数，当前间隔的本地计数和30秒前的本地计数）。然而，一个有趣的效果是，在失败后的未来30秒内的任何间隔可以表现出减速，因为它可能在丢失之前在30秒的间隔内显示局部计数的一部分。图5显示了这样一个例子，其中发生故障时30秒的间隔花费2.26秒来恢复，但是在时间32,34,46和48的间隔也稍长。我们计划从过去热切地推荐丢失的 RDD，以减轻这种损失。

![](/img/spark_paper/2012 Discretized Streams An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters/figure5.png)

# 5. Related Work

流处理的基础学术工作是在流数据库，如极光，北极星，电报和 STREAM。这些系统提供了一个 SQL 接口，并通过复制（活动或传递，前缀节点）或上游备份实现故障恢复。我们在这些系统上做出了两个贡献。首先，我们提供一个通用的编程接口，类似于 DryadLINQ，而不只是 SQL。第二，我们提供了一个更有效的恢复机制：并行重新计算丢失状态。由于 D-Streams 的确定性本质，并行恢复是可行的，这允许我们在其他节点上重新计算丢失的分区。相比之下，流 DB 对于每个输入记录更新可变状态，并且因此需要用于复制（例如，Flux）和上游备份\[12\]的复杂协议。 Hwang 等人所知的唯一的并行恢复协议只能容忍一个节点故障，并且不能处理stragglers（掉队）。

在工业中，大多数流处理框架使用较低级别的消息传递接口，其中用户编写状态代码以处理队列中的记录。示例包括 S4，Storm 和 Flume 。这些系统通常保证至少一次消息传递，但是与 D-Streams 不同，它们需要用户在故障（例如，通过保持复制的数据库中的所有状态）和节点间的一致性时手动地处理状态恢复。

最近的几个研究系统研究了在线集群中的在线处理。 MapReduce Online 是一个流式 Hadoop 运行时，但不能将多个 MapReduce 步骤组成一个查询或恢复有状态的 reduce 任务。 iMR 是用于日志处理的原位 MapReduce 引擎，但不支持更一般的计算图形，并且可能丢失失败数据。 CBP 和 Comet 通过每隔几分钟对新数据运行 MapReduce 作业来提供“批量增量处理”，以更新分布式文件系统中的状态;然而，它们引起复制磁盘存储的高开销。相比之下，D-Streams 可以在内存中保持状态，而不需要昂贵的复制，并且实现更低等级的延迟。 Naiad 递增地运行计算，但还没有集群实现或容错的讨论。 Percolator 使用触发器执行增量计算，但不提供跨节点或高级操作员（如 map 和 join）的一致性保证。

最后，我们的并行恢复机制在概念上类似于 MapReduce，GFS 和 RAMCloud 中的恢复技术，这些都利用重新分区。我们的贡献是表明这种机制可以应用在足够小的时间尺度进行流处理。此外，与 GFS 和 RAMCloud 不同，我们重新计算丢失的数据，而不是必须复制所有数据，避免了复制的网络和存储成本。

# 6. Conclusion

我们提出了离散流（D-Streams），一种用于大型集群的流编程模型，提供一致性，高效的故障恢复以及与批量系统的强大集成。关键思想是将流处理作为一系列短批处理作业，并尽可能降低这些作业的延迟。这给批处理模型带来了流处理的许多好处，包括清晰的一致性语义和一种新的并行恢复技术，我们认为这是第一个真正的成本效率高的恢复技术，用于大型集群中的流处理。我们的实现，Spark Streaming，让用户无缝地混合流，批处理和交互式查询。

在未来的工作中，我们计划使用Spark Streaming来构建集成这些类型的处理系统，并进一步探索 D-Streams 的限制。特别是，我们有兴趣推迟延迟甚至更低（约100毫秒）和通过提供近似结果更快地从故障中恢复。

# 7. Acknowledgements

这项研究得到了 NSF CISE Exversion 颁发的奖项，来自 Google，SAP，Amazon Web Services，Blue Goji，思科，Cloudera，爱立信，通用电气，惠普，华为，英特尔，Mark-微软，NetApp，Oracle，Quanta，Splunk和VMware，DARPA（合同＃FA8650-11-C-7136）和 Google 博士奖学金。











